{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b11c1-366e-4f16-b6b7-143c15abeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:33 PM] Piumika, Tinul\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    " \n",
    "\n",
    "# Generate dummy timestamps\n",
    "\n",
    "start_time = datetime(2023, 1, 1, 0, 0, 0)\n",
    "\n",
    "end_time = datetime(2023, 1, 10, 23, 0, 0)\n",
    "\n",
    "time_range = end_time - start_time\n",
    "\n",
    "timestamps = [start_time + timedelta(hours=i) for i in range(int(time_range.total_seconds() / 3600))]\n",
    "\n",
    " \n",
    "\n",
    "# Generate dummy OS and WebLogic metrics\n",
    "\n",
    "data = []\n",
    "\n",
    "for timestamp in timestamps:\n",
    "\n",
    "    cpu_usage = random.uniform(0, 100)  # Random CPU usage percentage\n",
    "\n",
    "    memory_usage = random.uniform(0, 100)  # Random memory usage percentage\n",
    "\n",
    "    disk_usage = random.uniform(0, 100)  # Random disk usage percentage\n",
    "\n",
    "    network_traffic = random.uniform(0, 1000)  # Random network traffic in KB/s\n",
    "\n",
    "    weblogic_requests = random.randint(0, 1000)  # Random WebLogic requests\n",
    "\n",
    "    weblogic_heap_usage = random.uniform(0, 100)  # Random WebLogic heap usage percentage\n",
    "\n",
    "    io_wait = random.uniform(0, 100)  # Random I/O wait percentage\n",
    "\n",
    "    thread_count = random.randint(1, 100)  # Random thread count\n",
    "\n",
    "    response_time = random.uniform(0, 1000)  # Random response time in ms\n",
    "\n",
    "    database_connections = random.randint(1, 50)  # Random database connections\n",
    "\n",
    "    jvm_gc_count = random.randint(0, 100)  # Random JVM garbage collection count\n",
    "\n",
    "    jvm_gc_time = random.uniform(0, 1000)  # Random JVM garbage collection time in ms\n",
    "\n",
    "    data.append({\n",
    "\n",
    "        'Timestamp': timestamp,\n",
    "\n",
    "        'CPU_Usage': cpu_usage,\n",
    "\n",
    "        'Memory_Usage': memory_usage,\n",
    "\n",
    "        'Disk_Usage': disk_usage,\n",
    "\n",
    "        'Network_Traffic': network_traffic,\n",
    "\n",
    "        'WebLogic_Requests': weblogic_requests,\n",
    "\n",
    "        'WebLogic_Heap_Usage': weblogic_heap_usage,\n",
    "\n",
    "        'IO_Wait': io_wait,\n",
    "\n",
    "        'Thread_Count': thread_count,\n",
    "\n",
    "        'Response_Time': response_time,\n",
    "\n",
    "        'Database_Connections': database_connections,\n",
    "\n",
    "        'JVM_GC_Count': jvm_gc_count,\n",
    "\n",
    "        'JVM_GC_Time': jvm_gc_time\n",
    "\n",
    "    })\n",
    "\n",
    " \n",
    "\n",
    "# Create DataFrame\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    " \n",
    "\n",
    "# Display the DataFrame\n",
    "\n",
    "#print(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7b0f7-6b0e-4386-a503-a299f7fe2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:34 PM] Piumika, Tinul\n",
    "\n",
    "# Feature Engineering Steps\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['ActLogTime'] = pd.to_datetime(df['Timestamp'], format='%H%M%S').dt.strftime(\"%Y-%m-%d %H:%M:%S\").str.split().str[1]\n",
    "\n",
    "#df['ActTime'] = df['ActLogTime'].apply(time_to_seconds)\n",
    "\n",
    "df[\"ActLogDate\"] = pd.to_datetime(pd.to_datetime(df['Timestamp'], format='%Y%m%d').dt.strftime(\"%Y-%m-%d %H:%M:%S\").str.split().str[0])\n",
    "\n",
    "df['Run_Period'] = df['ActLogTime'].apply(get_period)\n",
    "\n",
    "df['Weekday'] = df['ActLogDate'].apply(lambda x: 1 if x.weekday() < 5 else 0)\n",
    "\n",
    "df['ActMonth'] = df['ActLogDate'].dt.month\n",
    "\n",
    "df['ActDay'] = df['ActLogDate'].dt.day\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "time_labels = {}\n",
    "\n",
    "for i in range(60*24):\n",
    "\n",
    "    time_str = '{:02d}:{:02d}'.format(*divmod(i,60))\n",
    "\n",
    "    label = i % (60*24)\n",
    "\n",
    "    time_labels[time_str] = label\n",
    "\n",
    " \n",
    "\n",
    "df['ActLogTime'] = pd.to_datetime(df['ActLogTime'] , format = '%H:%M:%S')\n",
    "\n",
    "df['Time_label'] = df['ActLogTime'].apply(lambda x: time_labels[x.strftime('%H:%M')])\n",
    "\n",
    "df = df.apply(add_one_if_seconds_over_30, axis=1)\n",
    "\n",
    " \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5f7b9-b09b-4aa7-9d08-20a49c03a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:34 PM] Piumika, Tinul\n",
    "\n",
    "def min_max(df):\n",
    "\n",
    "    df['timestamp'] = df['Timestamp'].astype(int) // 10**9 \n",
    "\n",
    "    scaler = MinMaxScaler() \n",
    "\n",
    "    normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_data)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    new_names = []\n",
    "\n",
    "    for cname in df.columns:\n",
    "\n",
    "        new_names.append(f'{cname}_{i}')\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    #normalized_df = normalized_df.rename(columns={0: \"JOBNAME_0\", 1: \"ODateWeekNum_1\", 2:\"ODateWeekDay_2\", 3:\"HELD_3\", 4:\"FREED_4\"})\n",
    "\n",
    "    normalized_df.columns = new_names\n",
    "\n",
    "    return normalized_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea56ef9-915a-495c-abc6-5a720c46fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:34 PM] Piumika, Tinul\n",
    "\n",
    "JOBNAME_enc = LabelEncoder()\n",
    "\n",
    "df['Run_Period'] = JOBNAME_enc.fit_transform(df['Run_Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a05ae7-1946-48b5-bdc3-aa91907e5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:34 PM] Piumika, Tinul\n",
    "\n",
    "df_n, scaler = min_max(df)\n",
    "\n",
    "df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1353f-a9c3-4957-ab86-519f44dea264",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:35 PM] Piumika, Tinul\n",
    "\n",
    "def IsoForest(df):\n",
    "\n",
    "    #df['timestamp'] = df['Timestamp'].astype(int) // 10**9 \n",
    "\n",
    "    # Create an instance of the Isolation Forest model\n",
    "\n",
    "    isolation_forest = IsolationForest(n_estimators=1000, contamination=0.05)\n",
    "\n",
    " \n",
    "\n",
    "    # Fit the model to the data\n",
    "\n",
    "    isolation_forest.fit(df)\n",
    "\n",
    " \n",
    "\n",
    "    # Predict the anomalies\n",
    "\n",
    "    anomaly_scores = isolation_forest.decision_function(df)\n",
    "\n",
    "    anomaly_predictions = isolation_forest.predict(df)\n",
    "\n",
    "\n",
    "\n",
    "    # Add the anomaly scores and predictions to the original DataFrame\n",
    "\n",
    "    df['anomaly_score'] = anomaly_scores\n",
    "\n",
    "    df['anomaly_prediction'] = anomaly_predictions\n",
    "\n",
    " \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da036608-65be-48a3-8e04-4e65b6d867d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:35 PM] Piumika, Tinul\n",
    "\n",
    "iso_df = IsoForest(df_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447a268-9c2b-4605-9cfa-7e262cbc80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:35 PM] Piumika, Tinul\n",
    "\n",
    "def split_and_clean_df(df):\n",
    "\n",
    "\n",
    "    df_train = df[df['anomaly_prediction'] == 1].copy()\n",
    "\n",
    "    df_test = df[df['anomaly_prediction'] == -1].copy()\n",
    "\n",
    " \n",
    "\n",
    "    columns_to_remove = ['anomaly_score', 'anomaly_prediction']\n",
    "\n",
    "    df_train.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    df_test.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    " \n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424ea52-724a-4025-b00d-82b6f0c69ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:35 PM] Piumika, Tinul\n",
    "\n",
    "df_train, df_test = split_and_clean_df(iso_df)\n",
    "\n",
    "print(f'shape of training {df_train.shape}')\n",
    "\n",
    "print(f'shape of testing {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b0092-7944-4833-98ea-97453435ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:35 PM] Piumika, Tinul\n",
    "\n",
    "def autoencoder_prep(df_act):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Dataframe --> Matrix\")\n",
    "\n",
    "    matrix = df_act.to_numpy()\n",
    "\n",
    "\n",
    "    print(\"Setting Dimensional values\")\n",
    "\n",
    "    # input_dim = 5\n",
    "\n",
    "    # hidden_dim_1 = 4\n",
    "\n",
    "    # hidden_dim_2 = 3\n",
    "\n",
    "    # bottleneck_dim = 2\n",
    "\n",
    "    # hidden_dim_3 = 3\n",
    "\n",
    "    # hidden_dim_4 = 4\n",
    "\n",
    "    # output_dim = 5\n",
    "\n",
    "\n",
    "    input_dim = 21\n",
    "\n",
    "    hidden_dim_1 = 15\n",
    "\n",
    "    hidden_dim_2 = 9\n",
    "\n",
    "    bottleneck_dim = 4\n",
    "\n",
    "    hidden_dim_3 = 9\n",
    "\n",
    "    hidden_dim_4 = 15\n",
    "\n",
    "    output_dim = 21\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Specifying Layers of the Architecture\")\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim))\n",
    "\n",
    "\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(input_layer)\n",
    "\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(hidden_dim_2, activation='relu')(hidden_layer_1)\n",
    "\n",
    "\n",
    "    bottleneck_layer = tf.keras.layers.Dense(bottleneck_dim, activation='relu')(hidden_layer_2)\n",
    "\n",
    "\n",
    "    hidden_layer_3 = tf.keras.layers.Dense(hidden_dim_3, activation='relu')(bottleneck_layer)\n",
    "\n",
    "    hidden_layer_4 = tf.keras.layers.Dense(hidden_dim_4, activation='relu')(hidden_layer_3)\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation= 'sigmoid')(hidden_layer_4)\n",
    "\n",
    "\n",
    "    return matrix , input_layer , hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4 , output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5135b-0e35-4412-a5f3-2e25adca65ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee14c6b-aedd-469f-ae29-5986fdc619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:36 PM] Piumika, Tinul\n",
    "\n",
    "matrix , input_layer , hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4 , output_layer = autoencoder_prep(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb40d1-219f-404e-90e3-251d4aaec2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:37 PM] Piumika, Tinul\n",
    "\n",
    "#Training Process\n",
    "\n",
    "def autoencoder_train(input_layer, output_layer, matrix,  hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4, e):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Building the Model\")\n",
    "\n",
    "    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Training the Model\")\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder.fit(matrix, matrix, epochs=e, batch_size=64)\n",
    "\n",
    "    print(\"Successfully Trained the Model\")\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Calculate Reconstruction Error\")\n",
    "\n",
    "    reconstructed_data = autoencoder.predict(matrix)\n",
    "\n",
    "    mse = np.mean(np.power(matrix - reconstructed_data, 2), axis=1)\n",
    "\n",
    "    threshold = np.percentile(mse, 99.50) # Set threshold based on the 99.9th percentile\n",
    "\n",
    "    print(\"Threshold:\", threshold)\n",
    "\n",
    "\n",
    "    return autoencoder, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ec7bf-8cf5-4ac1-9d84-62f7b7a037f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:36 PM] Piumika, Tinul\n",
    "\n",
    "autoencoder, threshold = autoencoder_train(input_layer, output_layer, matrix,  hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6f866-b924-43e3-a162-7d3e6600ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:37 PM] Piumika, Tinul\n",
    "\n",
    "#Identifying Anomalies\n",
    "\n",
    "def autoencoder_anom(df_act, autoencoder, threshold, scaler):\n",
    "\n",
    "    print(\"Test Dataframe --> Matrix\")\n",
    "\n",
    " \n",
    "\n",
    "    X = df_act.to_numpy()\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Identifying Anomalies\")\n",
    "\n",
    " \n",
    "\n",
    "    normalized_test_data = X\n",
    "\n",
    "    reconstructed_test_data = autoencoder.predict(normalized_test_data)\n",
    "\n",
    " \n",
    "\n",
    "    test_mse = np.mean(np.power(normalized_test_data - reconstructed_test_data, 2), axis=1)\n",
    "\n",
    " \n",
    "\n",
    "    anomalies = X[test_mse > threshold]\n",
    "\n",
    " \n",
    "\n",
    "    df_anom = pd.DataFrame(anomalies, columns=df_act.columns)\n",
    "\n",
    "\n",
    "    df_anom[\"MSE\"] = test_mse[test_mse > threshold]\n",
    "\n",
    "\n",
    "    df_mse = pd.DataFrame(df_anom[\"MSE\"], columns = [\"MSE\"])\n",
    "\n",
    "    df_anom.drop(columns = ['MSE'], inplace = True)\n",
    "\n",
    "\n",
    "    df_anom = scaler.inverse_transform(df_anom)\n",
    "\n",
    " \n",
    "\n",
    "    return anomalies, df_anom, df_mse, normalized_test_data, reconstructed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e1cc0-6130-42d1-85b4-0edd5288e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:37 PM] Piumika, Tinul\n",
    "\n",
    "anomalies, df_anom, df_mse, normalized_test_data, reconstructed_test_data = autoencoder_anom(df_test, autoencoder, threshold, scaler)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"df_anom\")\n",
    "\n",
    "print(df_anom)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"MSE\")\n",
    "\n",
    "print(df_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437235e5-cc11-4f6c-915d-71000475871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:37 PM] Piumika, Tinul\n",
    "\n",
    "#Decoding\n",
    "\n",
    "def autoencoder_decode(df_anom,df):\n",
    "\n",
    "\n",
    "    df_anom = pd.DataFrame(df_anom)\n",
    "\n",
    "    #df_anom = df_anom.rename(columns={0: \"JOBNAME_0\", 1: \"ODateWeekNum_1\", 2:\"ODateWeekDay_2\", 3:\"HELD_3\", 4:\"FREED_4\"})\n",
    "\n",
    "    new_names = []\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for cname in df.columns:\n",
    "\n",
    "        #print(cname)\n",
    "\n",
    "        new_names.append(f'{cname}_{i}')\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    df_anom.columns = new_names\n",
    "\n",
    "    df_anom = df_anom.astype(int)\n",
    "\n",
    "    #Decoding\n",
    "\n",
    "    #print(new_names)\n",
    "\n",
    "    JOBNAME_enc = LabelEncoder()\n",
    "\n",
    "    df_anom['Run_Period_15'] = JOBNAME_enc.inverse_transform(df_anom['Run_Period_15'])\n",
    "\n",
    "\n",
    "    return df_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9198c31-2115-4919-91d6-0204f72e1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:38 PM] Piumika, Tinul\n",
    "\n",
    "df_anom = autoencoder_decode(df_anom,df)\n",
    "\n",
    "df_anom_mse = pd.concat([df_anom, df_mse], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c08349-8786-487a-908e-dadf96938141",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:38 PM] Piumika, Tinul\n",
    "\n",
    "#Explainable AI\n",
    "\n",
    "def autoencoder_insight(anomalies, normalized_test_data, reconstructed_test_data, df_anom):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Anomaly Identification Complete\")\n",
    "\n",
    "    print(\"Refer to df_anom for anomalies\")\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Explaining each Anomalies\")\n",
    "\n",
    "    feature_contributions = np.abs(normalized_test_data - reconstructed_test_data)\n",
    "\n",
    " \n",
    "\n",
    "    # Find the most important features for each anomaly\n",
    "\n",
    "    most_important_features = np.argsort(feature_contributions, axis=1)[:, ::-1]\n",
    "\n",
    " \n",
    "\n",
    "    # Print the most important features for each anomaly along with df_anom information\n",
    "\n",
    "    for i, anomaly in enumerate(anomalies):\n",
    "\n",
    "        print(f\"Anomaly {i + 1}:\")\n",
    "\n",
    "        print(df_anom.iloc[i])  # Print df_anom information for the current anomaly\n",
    "\n",
    "        print(\"Most important features:\")\n",
    "\n",
    "        for j, feature in enumerate(most_important_features[i]):\n",
    "\n",
    "            print(f\"   {j + 1}. Feature {feature}: Contribution = {feature_contributions[i][feature]}\")\n",
    "\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
