{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607625e5-3d3b-4bbf-9521-1602bfdf062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for model implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pytz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316e390-44c4-447d-98c2-6d11d8049353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for model implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pytz\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d17752-6bf9-40a0-bc83-1271489a1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beae57e-11fd-4170-b727-167a19660bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:28 PM] Piumika, Tinul\n",
    "\n",
    "# Function to convert logtime into a fraction of the day\n",
    "\n",
    "def fraction_of_day_to_hms(fraction): \n",
    "\n",
    "    seconds = int(round(86400 * fraction)) \n",
    "\n",
    "    hours, remainder = divmod(seconds, 3600) \n",
    "\n",
    "    minutes, seconds = divmod(remainder, 60) \n",
    "\n",
    "    return '{:02d}:{:02d}:{:02d}'.format(hours, minutes, seconds)\n",
    "\n",
    " \n",
    "\n",
    "# Function to extract last 4 words of a column (Targetting JobNames)\n",
    "\n",
    "def extract_last_4(JNAME):\n",
    "\n",
    "    words = JNAME.split('_')\n",
    "\n",
    "    last_4 = words[-4:]\n",
    "\n",
    "    return pd.Series(last_4)\n",
    "\n",
    " \n",
    "\n",
    "# Function to get running period (Morning, Afternoon, Evening, Night)\n",
    "\n",
    "def get_period(time_str):\n",
    "\n",
    "    hour = int(time_str.split(':')[0])\n",
    "\n",
    "    if hour >= 21 or hour < 9:\n",
    "\n",
    "        return 'Night'\n",
    "\n",
    "    elif hour < 13:\n",
    "\n",
    "        return 'Morning'\n",
    "\n",
    "    elif hour < 17:\n",
    "\n",
    "        return 'Afternoon'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Evening'\n",
    "\n",
    " \n",
    "\n",
    "def time_to_seconds(time_value):\n",
    "\n",
    "    time_str = str(time_value).strip()\n",
    "\n",
    "    hours = int(time_str[:2])\n",
    "\n",
    "    minutes = int(time_str[2:4])\n",
    "\n",
    "    seconds = int(time_str[4:6])\n",
    "\n",
    " \n",
    "\n",
    "    total_seconds = (hours * 3600) + (minutes * 60) + seconds\n",
    "\n",
    "    fraction = total_seconds/86400\n",
    "\n",
    " \n",
    "\n",
    "    return fraction\n",
    "\n",
    " \n",
    "\n",
    "def add_one_if_seconds_over_30(row):\n",
    "\n",
    "    time_obj = row['ActLogTime']\n",
    "\n",
    "    seconds = time_obj.second\n",
    "\n",
    "    if seconds >= 30:\n",
    "\n",
    "        row['Time_label'] += 1\n",
    "\n",
    "    return row\n",
    "\n",
    " \n",
    "\n",
    "def drop_date_part(df, column_name):\n",
    "\n",
    "    df[column_name] = df[column_name].dt.time\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8054eef-b1e6-496b-959d-dee747dd3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Steps\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['LogTimeSec'] = df['LOGTIME'].astype(str).str[-2:].astype(int)\n",
    "\n",
    "df['LOGTIME'] = df[\"LOGTIME\"].astype(str).apply(lambda x: x.zfill(6))\n",
    "\n",
    "df['ActTime'] = df['LOGTIME'].apply(time_to_seconds)\n",
    "\n",
    "df['ActLogTime'] = pd.to_datetime(df['LOGTIME'], format='%H%M%S').dt.strftime(\"%Y-%m-%d %H:%M:%S\").str.split().str[1]\n",
    "\n",
    "df[\"ActODAY\"] = pd.to_datetime(df['ODATE'], format='%Y%m%d').dt.strftime(\"%A\")\n",
    "\n",
    "df[\"ActODATE\"] = pd.to_datetime(df['ODATE'], format='%Y%m%d')\n",
    "\n",
    "df[\"MsgAction\"] = df['MESSAGE'].str.split().str[0]\n",
    "\n",
    "df['MsgAction'] = df['MsgAction'].replace('JOB','KILLED')\n",
    "\n",
    "df[\"MsgUser\"] = df['MESSAGE'].str.split().str[-1]\n",
    "\n",
    "df[\"ActLogDay\"] = pd.to_datetime(df['LOGDATE'], format='%Y%m%d').dt.strftime(\"%A\")\n",
    "\n",
    "df[\"ActLogDate\"] = pd.to_datetime(df['LOGDATE'], format='%Y%m%d')\n",
    "\n",
    "df[\"ActODate\"] = pd.to_datetime(df['ODATE'], format='%Y%m%d')\n",
    "\n",
    "df['ODateWeekNum'] = pd.to_datetime(df['ODATE'], format='%Y%m%d').apply(lambda x: x.isocalendar()[1])\n",
    "\n",
    "df['ODateWeekDay'] = pd.to_datetime(df['ODATE'], format='%Y%m%d').dt.weekday+1\n",
    "\n",
    "df['Run_Period'] = df['ActLogTime'].apply(get_period)\n",
    "\n",
    "df['LogDateDiff'] = (df[\"ActLogDate\"] - df[\"ActODATE\"]).dt.days\n",
    "\n",
    "df['NewLogTime'] = df['LOGTIME'].apply(int) + (df['LogDateDiff']*24*10000)\n",
    "\n",
    "df['Weekday'] = df['ActODATE'].apply(lambda x: 1 if x.weekday() < 5 else 0)\n",
    "\n",
    "df['ActMonth'] = df['ActODATE'].dt.month\n",
    "\n",
    "df['ActDay'] = df['ActODATE'].dt.day\n",
    "\n",
    "df.drop('ActLogDate',axis = 1, inplace=True)\n",
    "\n",
    " \n",
    "\n",
    "time_labels = {}\n",
    "\n",
    "for i in range(60*24):\n",
    "\n",
    "    time_str = '{:02d}:{:02d}'.format(*divmod(i,60))\n",
    "\n",
    "    label = i % (60*24)\n",
    "\n",
    "    time_labels[time_str] = label\n",
    "\n",
    " \n",
    "\n",
    "df['ActLogTime'] = pd.to_datetime(df['ActLogTime'] , format = '%H:%M:%S')\n",
    "\n",
    "df['Time_label'] = df['ActLogTime'].apply(lambda x: time_labels[x.strftime('%H:%M')])\n",
    "\n",
    "df = df.apply(add_one_if_seconds_over_30, axis=1)\n",
    "\n",
    " \n",
    "\n",
    "df = drop_date_part(df, 'ActLogTime')\n",
    "\n",
    " \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9036bf6-6f3e-4a67-bdb5-114d6395bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:29 PM] Piumika, Tinul\n",
    "\n",
    "def min_max(df):\n",
    "\n",
    "    scaler = MinMaxScaler() \n",
    "\n",
    "    normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_data)\n",
    "\n",
    "    normalized_df = normalized_df.rename(columns={0: \"JOBNAME_0\", 1: \"ODateWeekNum_1\", 2:\"ODateWeekDay_2\", 3:\"HELD_3\", 4:\"FREED_4\"})\n",
    "\n",
    "    return normalized_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf7162-ba75-4b47-9d1a-376a6178cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:29 PM] Piumika, Tinul\n",
    "\n",
    "def IsoForest(df):\n",
    "\n",
    " \n",
    "\n",
    "    # Create an instance of the Isolation Forest model\n",
    "\n",
    "    isolation_forest = IsolationForest(n_estimators=1000, contamination=0.05)\n",
    "\n",
    " \n",
    "\n",
    "    # Fit the model to the data\n",
    "\n",
    "    isolation_forest.fit(df)\n",
    "\n",
    " \n",
    "\n",
    "    # Predict the anomalies\n",
    "\n",
    "    anomaly_scores = isolation_forest.decision_function(df)\n",
    "\n",
    "    anomaly_predictions = isolation_forest.predict(df)\n",
    "\n",
    "\n",
    "\n",
    "    # Add the anomaly scores and predictions to the original DataFrame\n",
    "\n",
    "    df['anomaly_score'] = anomaly_scores\n",
    "\n",
    "    df['anomaly_prediction'] = anomaly_predictions\n",
    "\n",
    " \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3d9cd-6310-4913-8cd9-f4b1c454ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:29 PM] Piumika, Tinul\n",
    "\n",
    "def split_and_clean_df(df):\n",
    "\n",
    "\n",
    "    df_train = df[df['anomaly_prediction'] == 1].copy()\n",
    "\n",
    "    df_test = df[df['anomaly_prediction'] == -1].copy()\n",
    "\n",
    " \n",
    "\n",
    "    columns_to_remove = ['anomaly_score', 'anomaly_prediction']\n",
    "\n",
    "    df_train.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    df_test.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    " \n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031caed0-c42a-44ea-b04a-d5236f625a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:30 PM] Piumika, Tinul\n",
    "\n",
    "def autoencoder_prep(df_act):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Dataframe --> Matrix\")\n",
    "\n",
    "    matrix = df_act.to_numpy()\n",
    "\n",
    "\n",
    "    print(\"Setting Dimensional values\")\n",
    "\n",
    "    input_dim = 5\n",
    "\n",
    "    hidden_dim_1 = 4\n",
    "\n",
    "    hidden_dim_2 = 3\n",
    "\n",
    "    bottleneck_dim = 2\n",
    "\n",
    "    hidden_dim_3 = 3\n",
    "\n",
    "    hidden_dim_4 = 4\n",
    "\n",
    "    output_dim = 5\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Specifying Layers of the Architecture\")\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim))\n",
    "\n",
    "\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(input_layer)\n",
    "\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(hidden_dim_2, activation='relu')(hidden_layer_1)\n",
    "\n",
    "\n",
    "    bottleneck_layer = tf.keras.layers.Dense(bottleneck_dim, activation='relu')(hidden_layer_2)\n",
    "\n",
    "\n",
    "    hidden_layer_3 = tf.keras.layers.Dense(hidden_dim_3, activation='relu')(bottleneck_layer)\n",
    "\n",
    "    hidden_layer_4 = tf.keras.layers.Dense(hidden_dim_4, activation='relu')(hidden_layer_3)\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation= 'sigmoid')(hidden_layer_4)\n",
    "\n",
    "\n",
    "    return matrix , input_layer , hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4 , output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f377b1-e27d-45b3-b88a-11f77733d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:30 PM] Piumika, Tinul\n",
    "\n",
    "#Training Process\n",
    "\n",
    "def autoencoder_train(input_layer, output_layer, matrix,  hidden_layer_1 , hidden_layer_2 , bottleneck_layer , hidden_layer_3 , hidden_layer_4, e):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Building the Model\")\n",
    "\n",
    "    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Training the Model\")\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder.fit(matrix, matrix, epochs=e, batch_size=64)\n",
    "\n",
    "    print(\"Successfully Trained the Model\")\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Calculate Reconstruction Error\")\n",
    "\n",
    "    reconstructed_data = autoencoder.predict(matrix)\n",
    "\n",
    "    mse = np.mean(np.power(matrix - reconstructed_data, 2), axis=1)\n",
    "\n",
    "    threshold = np.percentile(mse, 99.50) # Set threshold based on the 99.9th percentile\n",
    "\n",
    "    print(\"Threshold:\", threshold)\n",
    "\n",
    "\n",
    "    return autoencoder, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc0aa1-b78a-4428-a3a9-b8e4b61f0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:30 PM] Piumika, Tinul\n",
    "\n",
    "#Identifying Anomalies\n",
    "\n",
    "def autoencoder_anom(df_act, autoencoder, threshold, scaler):\n",
    "\n",
    "    print(\"Test Dataframe --> Matrix\")\n",
    "\n",
    " \n",
    "\n",
    "    X = df_act.to_numpy()\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Identifying Anomalies\")\n",
    "\n",
    " \n",
    "\n",
    "    normalized_test_data = X\n",
    "\n",
    "    reconstructed_test_data = autoencoder.predict(normalized_test_data)\n",
    "\n",
    " \n",
    "\n",
    "    test_mse = np.mean(np.power(normalized_test_data - reconstructed_test_data, 2), axis=1)\n",
    "\n",
    " \n",
    "\n",
    "    anomalies = X[test_mse > threshold]\n",
    "\n",
    " \n",
    "\n",
    "    df_anom = pd.DataFrame(anomalies, columns=df_act.columns)\n",
    "\n",
    "\n",
    "    df_anom[\"MSE\"] = test_mse[test_mse > threshold]\n",
    "\n",
    "\n",
    "    df_mse = pd.DataFrame(df_anom[\"MSE\"], columns = [\"MSE\"])\n",
    "\n",
    "    df_anom.drop(columns = ['MSE'], inplace = True)\n",
    "\n",
    "\n",
    "    df_anom = scaler.inverse_transform(df_anom)\n",
    "\n",
    " \n",
    "\n",
    "    return anomalies, df_anom, df_mse, normalized_test_data, reconstructed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bcf0b-556e-4d1a-b300-6b228b7b3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:30 PM] Piumika, Tinul\n",
    "\n",
    "#Decoding\n",
    "\n",
    "def autoencoder_decode(df_anom):\n",
    "\n",
    "\n",
    "    df_anom = pd.DataFrame(df_anom)\n",
    "\n",
    "    df_anom = df_anom.rename(columns={0: \"JOBNAME_0\", 1: \"ODateWeekNum_1\", 2:\"ODateWeekDay_2\", 3:\"HELD_3\", 4:\"FREED_4\"})\n",
    "\n",
    "    df_anom = df_anom.astype(int)\n",
    "\n",
    "    #Decoding\n",
    "\n",
    "\n",
    "    #JOBNAME_enc = LabelEncoder()\n",
    "\n",
    "    df_anom['JOBNAME_0'] = JOBNAME_enc.inverse_transform(df_anom['JOBNAME_0'])\n",
    "\n",
    "\n",
    "    return df_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a82bd5-73ab-43b7-b959-1cadf23838b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[9:31 PM] Piumika, Tinul\n",
    "\n",
    "#Explainable AI\n",
    "\n",
    "def autoencoder_insight(anomalies, normalized_test_data, reconstructed_test_data, df_anom):\n",
    "\n",
    " \n",
    "\n",
    "    print(\"Anomaly Identification Complete\")\n",
    "\n",
    "    print(\"Refer to df_anom for anomalies\")\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Explaining each Anomalies\")\n",
    "\n",
    "    feature_contributions = np.abs(normalized_test_data - reconstructed_test_data)\n",
    "\n",
    " \n",
    "\n",
    "    # Find the most important features for each anomaly\n",
    "\n",
    "    most_important_features = np.argsort(feature_contributions, axis=1)[:, ::-1]\n",
    "\n",
    " \n",
    "\n",
    "    # Print the most important features for each anomaly along with df_anom information\n",
    "\n",
    "    for i, anomaly in enumerate(anomalies):\n",
    "\n",
    "        print(f\"Anomaly {i + 1}:\")\n",
    "\n",
    "        print(df_anom.iloc[i])  # Print df_anom information for the current anomaly\n",
    "\n",
    "        print(\"Most important features:\")\n",
    "\n",
    "        for j, feature in enumerate(most_important_features[i]):\n",
    "\n",
    "            print(f\"   {j + 1}. Feature {feature}: Contribution = {feature_contributions[i][feature]}\")\n",
    "\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
